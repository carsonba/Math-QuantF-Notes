\documentclass[../main.tex]{subfiles}
\begin{document}



\section{Probability}

\begin{remark}
To build a mathematical model of uncertainty and randomness we start by thinking what it is we will be measuring. We want to be able to measure the likelihood of some event happening. So we will measure events. What's an event though? It is an occurrence of something and this something is comprised of smaller events that the event we are concerned with is comprised of. These smaller events we will call outcomes, or singleton event. So for example, lets say I want to know the probability that my plane crashes. The event of a plane crash is composed of infinitely many outcomes... the event maybe consists of an outcome where a mechanic overlooked something, then something wobbled in just the right way, then the plane took a certain turn which caused some screw to loosen, ..., then the plane crashed. So the screw loosening is one outcome, the plane turning is another, etc. all these outcomes make up the event where a plane crashes. So we will consider sets of outcomes, we will call these events. 


\end{remark}



\begin{remark}
The below theorems will show us how to build these events, how to count the number of outcomes in the events. 
\end{remark}



\begin{theorem}
Let \( X_1, X_2, \dots, X_n \) be finite sets with cardinalities \( |X_1|, |X_2|, \dots, |X_n| \). If a process consists of making sequential choices such that:
\begin{itemize}
    \item The first choice is made from \( X_1 \),
    \item The second choice is made from \( X_2 \),
    \item \(\dots\),
    \item The \( n \)th choice is made from \( X_n \),
\end{itemize}
where the number of choices at each stage is independent of previous choices, then the total number of ways to complete the process is:
\[
|X_1| \cdot |X_2| \cdots |X_n| = \prod_{i=1}^{n} |X_i|.
\]
\end{theorem}


\begin{theorem} \label{thm:binomial coefficient}
Let n and k be nonnegative integers with \( 0 \leq k \leq n\). The number of distinct subsets of size \(k\) that a set of size \(n\) has is given by the binomial coefficient
\[
\binom{n}{k} = \frac{n!}{k!(n-k)!}
\]

\end{theorem}

\begin{theorem} \label{thm:binomial theorem}
For any integer \(n \geq 0\) and any real or complex numbers a,b, 
\[
(a+b)^n = \sum^n_{k=0}{\binom{n}{k} a^{k}b^{n-k}},
\]


\end{theorem}


\begin{theorem} \label{thm:permutations}
The number of ways to arrange n distinct objects in a sequence is 
\[
P(n) = n! = n(n-1)(n-2) \cdots 2 \cdot 1
\]
The number of ways to select and arrange k objects from n distinct objects is
\[
P(n,k) = \frac{n!}{(n-k)!}.
\]
\end{theorem}


\subsection{Axioms of Probability}

\begin{remark}
So how will we define the abstract space we will be working in so that we can effectively measure the likelihood of events? So consider some event $A$ that you want to know the likelihood of. If the event $A$ is possible, then it should also be possible that $A^c$, meaning, we should be able to measure both of these. So lets call the space $\mathcal{A}$, we will add sets to $\mathcal{A}$ that we think should be possible to measure if it is to be possible to measure $A$. So we have $A \in \mathcal{A}$ and $ A^c \in \mathcal{A} $. If $A \in \mathcal{A}$ and $ B \in \mathcal{A} $ then we should be able to measure $ A \cup B $ and $ A \cap B $, so we include all intersections and unions of possible events in $\mathcal{A}$.
\end{remark}



\begin{definition}[Algebra and $\sigma$-algebra]
Let $\Omega$ be an abstract space. Let $2^{\Omega}$ denote all subsets of $\Omega$. With $\mathcal{A}$ being a subset of $2^\Omega$. Then $\mathcal{A}$ is an algebra if it satisfies $(1),(2),$ and $(3)$. $\mathcal{A}$ is a $\sigma$-algebra if it satisfies $(1), (2),$ and $(4)$.
\begin{enumerate}
    \item $\emptyset \in \mathcal{A}$ and $\Omega \in \mathcal{A}$
    \item If $ A \in \mathcal{A}$ then $A^c \in \mathcal{A}$.
    \item If the finite sequence of events $A_1, A_2, \dots ,A_n \in \mathcal{A}$ then $\bigcup^n_{i=1}{A_i} \in \mathcal{A}$ and $\bigcap^n_{i=1}{A_i} \in \mathcal{A}$.
    \item If the countable sequence of events $A_1, A_2, \dots \in \mathcal{A}$ then $\bigcup^\infty_{i=1}{A_i} \in \mathcal{A}$ and $ \bigcap^\infty_{i=1}{A_i} \in \mathcal{A}    $. 
\end{enumerate}
    
\end{definition}



\begin{remark}
If $ C \subset 2^\Omega$, then the $\sigma$-algebra generated by $C$, denoted $ \sigma(C) $, is the smallest $ \sigma $-algebra containing $C$.
\end{remark}

We choose $b_n$ to be strictly increasing so that the $ ,b_n]$ part of the interval $ (a_n, b_n] $ converges to $ , b_n)$. This is what allows us to have that $(a,b) = \bigcup^\infty_{n=1}{(a_n, b_n]}$. 



\begin{theorem}[Borel $\sigma$-algebra]
If $\Omega = \mathbb{R}$, the Borel $\sigma$-algebra is the $\sigma$-algebra generated by open sets (or equivalently closed sets). Then the Borel $\sigma $-algebra can be generated by intervals of the form $ (-\infty, a] $, where $ a \in \mathbb{Q} $.
\end{theorem}

\begin{proof}
Let $C$ denote all open intervals. Since every open set in $ \mathbb{R} $ is the countable union of open intervals, we have $ \sigma(C) =  $ the Borel $ \sigma$-algebra of $\mathbb{R}$. Let $D$ denote all intervals of the form $(-\infty, a]$, where $ a \in \mathbb{Q}$. Let $ (a,b) \in C$, and let $ (a_n)^\infty_{n \geq 1} $ be a sequence of rationals decreasing to $a$ and $(b_n)^\infty_{n\geq1}$ be a sequence of rationals strictly increasing to $b$. Then
\[
(a,b) = \bigcup^\infty_{n=1}{(a_n, b_n]} = \bigcup^\infty_{n=1}{((-\infty, b_n] \cap (a_n, \infty))} = 
\bigcup^\infty_{n=1}{((-\infty, b_n] \cap (-\infty, a_n]^c)}
\]
Since the right most expression is of the form of $D$ and since we have that any element of $C$ is equivalent to an element of $D$, we have $ C \subset \sigma(D) $, hence $ \sigma(C) \subset \sigma(D) $. However since $ (-\infty, a]$ contains all its limit points, we know each element of $D$ is a closed set, since closed sets are Borel sets, we have that $\sigma(D)$ is contained in the Borel sets $ \mathcal{B}$. Thus we have
\[
\mathcal{B} = \sigma(C) \subset \sigma(D) \subset \mathcal{B},
\]
and hence $ \sigma(D) = \mathcal{B}.$
\end{proof}

\begin{remark}
So the theorem above shows that when our sample space is the real numbers, or any space with the proper topology, we can generate the $\sigma$-algebra we define the probability measure on by using intervals of the form $(-\infty, a]$, where $ a \in \mathbb{Q}$. Since $ a \in \mathbb{Q}$, we have made the $\sigma$-algebra from countable sets. Which is what we needed since before we knew that open sets $ \mathcal{C}$ could cover any set, we had to show that the countable collection of $(-\infty, a]$ could also cover (and thus measure) any set.
\end{remark}



\begin{remark}
For our actual probability measure, we need an event that is guaranteed and an event that is impossible, so we include $ \emptyset \in \mathcal{A} $ and $ \Omega \in \mathcal{A} $ since we want $ P(\emptyset) = 0 $ and $ P(\Omega) = 1 $. We also would want that if we measure two events $ A$ and $B$ where $A$ and $B$ share no outcomes $A \cap B = \emptyset$, then we want that $ P(A \cup B) = P(A) + P(B) $.
\end{remark}





\begin{definition}[Probability Measure]
A probability measure defined on a $ \sigma $-algebra $\mathcal{A}$ of $\Omega$ is a function $ P: \mathcal{A} \to [0,1] $ that satisfies 
\begin{enumerate}
    \item $P(\Omega)=1$
    \item For every pairwise disjoint $ (A_n \cap A_m = \emptyset \textnormal{ whenever } n \neq m) $ countable sequence $ (A_n)_{n \geq 1} $ of elements of $\mathcal{A}$, we have 
    \[
    P\left(\bigcup^\infty_{n=1}{A_n}\right) = \sum^\infty_{n=1}{P(A_n)}.
    \]
\end{enumerate}
\end{definition}







\begin{definition}
If $A \in 2^\Omega$, then the indicator function $1_A(\omega)$ be given by
\[
1_A(\omega) = 
\begin{cases}
    1 \textit{ if } \omega \in A,\\
    0 \textit{ if } \omega \notin A.
\end{cases}
\]
We say $A_n \in \mathcal{A}$ converges to $A$ if $\lim_{n\to\infty}{1_{A_n}(\omega)} = 1_A(\omega)$ $\forall \omega \in \Omega$.
\end{definition}

\begin{remark}
So if we can define convergence of a sequence of sets, then we must have some conception of 
convergene of supremum and infimum. How will we define these? We want the supremum of a set to be 
\end{remark}

\begin{definition}
If $A_n \in \mathcal{A}$ $\forall n \in \mathbb{N}$ then define
\begin{gather*}
\limsup_{n\to\infty}{A_n} = \cap^\infty_{n=1}\cup_{m \geq n}{A_m}\\
\liminf_{n\to\infty}{A_n} = \cup^\infty_{n=1}\cap_{m\geq n}{A_m}.
\end{gather*}

\end{definition}


\begin{theorem}
Let $P$ be a probability measure, and let $A_n$ be a sequence of events in $\mathcal{A}$ which converges
to $A$. Then $A \in \mathcal{A}$ and $\lim_{n \to \infty}{P(A_n)} = P(A)$.
\end{theorem}

\begin{proof}

\end{proof}

\begin{theorem}
Let \(A_1, A_2, \dots, A_n\) be events, then
\begin{enumerate}
    \item \( P(A \cup B) = P(A) + P(B) - P(A\cap B)\)
    \item \( P(A \cup B \cup C) = P(A) + P(B) + P(C) - \left(P(A \cap B) + P(A \cap C) + P(B \cap C)\right) + P(A \cap B \cap C). \)
    \item \( P(A_1 \cup A_2 \cup \cdots \cup A_n) = \sum_{i=1}^{n}{P(A_i)} - \sum_{1 \leq i_1 \leq i_2 \leq n}{P(A_{i_1} \cap A_{i_2})}\\ + \sum_{1 \leq i_1 \leq i_2 \leq i_3 \leq n}{P(A_{i_1} \cap A_{i_2} \cap A_{i_3})}\\ - \sum_{1 \leq i_1 < i_2 \leq i_3 < i_4 \leq n}{P(A_{i_1} \cap A_{i_2} \cap A_{i_3} \cap A_{i_4})} \\ + \cdots + (-1)^{n+1}P(A_1 \cap \cdots \cap A_n)  = \sum_{k=1}^{n}(-1)^{k+1} \sum_{1 \leq i_1 < \cdots < i_k \leq n}{P(A_{i_1} \cap \cdots \cap A_{i_k})} \)
\end{enumerate}

\end{theorem}



\begin{exercise}
Let $A_i \in \mathcal{A}$ be a sequence of events. Show that 
\[
P(\cup^n_{i=1}{A_i}) \leq \sum_{i=1}^{\infty}{P(A_i)} 
\]
\end{exercise}





\begin{definition} \label{def: conditional_prob}
    

Let B be an event in the sample space \(\Omega\) such that \(P(B) > 0\). Then for all events \(A\) the \textit{conditional probability} of \(A\) given \(B\) is defined as
\[
P(A \mid B) = \frac{P(A \cap B)}{P(B)}.
\]

\end{definition}

\begin{definition}\label{def: independence}
A collection of events $(A_i)_{i \in I}$ is an independent collection if for every finite subset $J$ of $I$, one has 
\[
P(\cap_{i \in J}{A_i}) = \prod_{i \in J}{P(A_i)}.
\]
If the above condition is satisfied for for the whole collection, we say the collection $(A_i)_{i \in I}$ is mutually independent. Also, 
if $A_i$ and $A_j$ are independent $\forall i,j$ with $i \neq j$, that is if any two events you pick from the collection $(A_i)_{i \in I}$ are independent, then the collection is pariwise independent.

\end{definition}



\begin{exercise}
If $A$ and $B$ are independent, so also are $A$ and $B^c$, $A^c$ and $B$, and $A^c$ and $B^C$. 
\end{exercise}

\begin{proposition}
If \(A_1, A_2, \dots, A_n \in \mathcal{A}\) and if $ P(A_1 \cap \cdots \cap A_{n-1}) > 0$, then  
\[
P(A_1 \cap A_2 \cap \dots \cap A_n) = P(A_1)P(A_2 \mid A_1)P(A_3 \mid A_1 \cap A_2) \dots P(A_n \mid A_1 \cap \dots \cap A_n).
\]
\end{proposition}


\begin{definition}
A countable collection of events \(B_1, \dots, B_n\) are a \textit{partition} of \(\Omega\) 
if the sets \(B_i\) are pairwise disjoint and together they make up \(\Omega\). 
That is, for all i and j, \( B_i \cap B_j = \emptyset \) whenever \(i \neq j\) 
and \( \bigcup_{i=1}^n B_i = \Omega\)
\end{definition}

\begin{proposition}
    Suppose that \(B_1, \dots, B_n\) is a partition of \(\Omega\) with \(P(B_i) > 0\) for \(i = 1, \dots, n.\) Then for any event A we have
    \[
    P(A) = \sum_{i=1}^n P(A \cap B_i) = \sum_{i=1}^n P(A \mid B_i)P(B_i).
    \]
    
\end{proposition}


\begin{theorem}\label{thm:bayes_formula}
Let \( B_1, B_2, \dots, B_n \) be a partition of the sample space \( \Omega \) such that each \( P(B_i) > 0 \). Then for any event \( A \) with \( P(A) > 0 \), and for any \( k = 1, \dots, n \), we have:
\[
P(B_k \mid A) = \frac{P(A B_k)}{P(A)} = \frac{P(A \mid B_k) P(B_k)}{\sum_{i=1}^{n} P(A \mid B_i) P(B_i)}.
\]
\end{theorem}





\begin{definition}\label{def:conditional_independence}
Let \( A_1, A_2, \dots, A_n \) and \( B \) be events with \( P(B) > 0 \). Then \( A_1, A_2, \dots, A_n \) are \textit{conditionally independent, given \( B \)}, if the following condition holds:

For any \( k \in \{2, \dots, n\} \) and indices \( 1 \leq i_1 < i_2 < \dots < i_k \leq n \),
\[
P(A_{i_1} A_{i_2} \dots A_{i_k} \mid B) = P(A_{i_1} \mid B) P(A_{i_2} \mid B) \cdots P(A_{i_k} \mid B).
\]
\end{definition}






\begin{theorem}
(a) A probability on the countable set $\Omega$ is characterized by its values on the atoms: 
\[
p_\omega = P({\omega}), \quad \omega \in \Omega.
\]
(b) Let $(p_\omega)_{\omega \in \Omega}$ be a family of real numbers indexed by $\Omega$.
Then there exists a unique probability $P$ such that $P({\omega}) = p_\omega$ if and only if $p_\omega \geq 0$ and $\sum_{\omega \in \Omega}{p_\omega} = 1$
\end{theorem}


\begin{definition} \label{def: Random_Variable}
A random variable is a measurble function $ X: \Omega \to \mathbb{R}$ such that for all Borel measurable sets $ B \subseteq \mathbb{R}$,
the preimage of $B$ is an event in $\mathcal{A}$, that is 
\[
X^{-1}(B) = \{ \omega \in \Omega \mid X(\omega) \in B \} \in \mathcal{F}.
\]
This means that $X$ is $\mathcal{A}$-measurable, ensuring that we can compute probabilities of the form $P(X \in B)$
\end{definition}


\begin{remark}
So a random variable inputs events or outcomes and outputs a real number, then the probability measure will assign probabilities in $[0,1]$ to the values of $X$.
We can then define the distribution of $X$ by
\[
P^X(A) = P({\omega \mid X(w) \in A}) = P(X^{-1}(A)) = P(X \in A)
\]
This is completely determined by the following 
\[
p_j^X = P(X=j) = \sum_{\omega \mid X(w) =j} \textnormal{ and } P_X(A) = \sum_{j \in A}{p^X_j}
\]
\end{remark}

\begin{definition}
Let $X$ be a real-valued random variable on a countable space $\Omega$. The expectation of $X$, denoted $E(X)$, is defined to be 
\[
E(X) = \sum_{\omega}{X(\omega)p_\omega} \quad \textnormal{or } \quad \int_{-\infty}^{\infty}X(\omega)p_\omega d\omega??
\]
provided this sum converges. Notice that if the random variable is discrete we use the finite sum, if it is continuous, we use the continuous sum.
\end{definition}

\begin{definition}
The $n$th moment of the random variable $X$ is the expectation $E(X^n)$. 
\[
E(X^n) = \sum_{\omega}{X^n(\omega)p_\omega} \quad \textnormal{or } \quad \int_{-\infty}^{\infty}X^n(\omega)P(X(\omega)) d\omega
\]
\end{definition}



\begin{theorem}
Let $h: \mathbb{R} \to [0,\infty)$ be a nonnegative function and let $X$ be a real valued random variable. Then 
\[
P(\{\omega \mid h(X(\omega)) \geq a\}) \leq \frac{E(H(X))}{a}, \quad \forall a > 0.
\]
\end{theorem}



\begin{corollary}[Markovs Inequality] \label{Markovs Inequality}
\[
P({|X| \geq a}) \leq \frac{E(|X|)}{a}
\]

\end{corollary}



\begin{definition}\label{def: Variance and Standard Deviation}
Let $X$ be a real valued random variable with $X^2 \in \mathcal{L}^1$ where $\mathcal{L}^1$ is the space of real valued random variables on $(\Omega, \mathcal{A}, P)$. The variance of $X$ is defined to be
\[
\sigma^2 = \sigma^2_X = E((X - E(X))^2) = E(X^2) - (E(X))^2
\]
The standard deviation of $X$, $\sigma_X$, is the nonnegative square root of the variance. 

\end{definition}






\begin{corollary}[\textbf{Chebyshev's Inequality}]
If $X^2$ is in $\mathcal{L}^1$, then for $a>0$ we have
\begin{enumerate}
    \item $ \quad P(\{|X| \geq a\}) \leq \frac{E{X^2}}{a^2}$
    \item $ \quad P(\{|X - E(X)| \geq a\}) \leq \frac{\sigma^2_X}{a^2}$
\end{enumerate}

\end{corollary}


\begin{definition}[\textbf{Binomial Distribution}]
Let $n$ be a positive integer and $ 0 \leq p \leq 1$. A random variable $X$ has the $binomial distribution$ with parameters
$n$ and $p$ if the possible values of $X$ are $\{0,1,\dots,n\}$ and the probabilities are 
\[
P(\{X=k\}) = \binom{n}{k}p^k(1-p)^{n-k} \quad \textnormal{for } k = 0,1,\dots n.
\]
This is denoted $X ~ Bin(n,p)$.
    
\end{definition}


\begin{definition}[\textbf{Geometric Distribution}]
A random variable \( X \) follows a Geometric distribution with parameter \( p \) (success probability per trial) if the probability of $k$ independent trials till a success on the $k$th trial is given by,

\[
P(X = k) = (1 - p)^{k-1} p, \quad k = 1, 2, 3, \dots
\]
\end{definition}


\begin{definition}[\textbf{Hypergeometric Distribution}]
A hypergeometric random variable represents the number of successes of size $n$, drawn without replacement from a population of size \( N \) that contains \( K \) successes. The PMF is given by
\[
P(X = k) = \frac{\binom{K}{k} \binom{N - K}{n - k}}{\binom{N}{n}}, \quad \max(0, n - (N - K)) \leq k \leq \min(n, K).
\]

\end{definition}

\begin{definition}[\textbf{Poisson Distribution}]
A Poisson random variable models the number of events occurring in a fixed interval of time or space, under the assumption that events occur independently and at a constant average rate \( \lambda \).
A random variable \( X \) follows a Poisson distribution with rate parameter \( \lambda > 0 \) if

\[
P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}, \quad k = 0, 1, 2, \dots.
\]


\end{definition}


\begin{definition}[\textbf{Normal Distribution}]
A random variable \( X \) follows a Normal distribution with mean \( \mu \) and variance \( \sigma^2 \), written as \( X \sim \mathcal{N}(\mu, \sigma^2) \), if its probability density function (PDF) is
\[
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(x - \mu)^2}{2\sigma^2} \right), \quad x \in \mathbb{R}.
\]
\end{definition}




































































































\end{document}