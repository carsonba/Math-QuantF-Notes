\documentclass[../main.tex]{subfiles}
\begin{document}

\section{Linear Algebra}



\begin{definition}
    Let \( F \) be a field. A \textbf{vector space} over \( F \) is a set \( V \) equipped with two operations:
    \begin{itemize}
        \item \textbf{Vector addition}: A function \( +: V \times V \to V \) assigning to each pair \( (v, w) \in V \times V \) a sum \( v + w \in V \).
        \item \textbf{Scalar multiplication}: A function \( \cdot: F \times V \to V \) assigning to each scalar \( a \in F \) and vector \( v \in V \) a product \( av \in V \).
    \end{itemize}
    These operations satisfy the following axioms for all \( u, v, w \in V \) and all \( a, b \in F \):
        
    \begin{enumerate}
        \item \textbf{Axioms for Vector Addition:}
        \begin{enumerate}
            \item \textbf{Closure}: \( v + w \in V \).
            \item \textbf{Associativity}: \( u + (v + w) = (u + v) + w \).
            \item \textbf{Commutativity}: \( v + w = w + v \).
            \item \textbf{Existence of Additive Identity}: There exists an element \( 0 \in V \) such that \( v + 0 = v \) for all \( v \in V \).
            \item \textbf{Existence of Additive Inverses}: For each \( v \in V \), there exists \( -v \in V \) such that \( v + (-v) = 0 \).
        \end{enumerate}
            
        \item \textbf{Axioms for Scalar Multiplication:}
        \begin{enumerate}
                \item \textbf{Closure}: \( av \in V \) for all \( a \in F \) and \( v \in V \).
                \item \textbf{Distributivity over Vector Addition}: \( a(v + w) = av + aw \).
                \item \textbf{Distributivity over Scalar Addition}: \( (a + b)v = av + bv \).
                \item \textbf{Associativity}: \( (ab)v = a(bv) \).
                \item \textbf{Multiplicative Identity}: There exists a scalar \( 1 \in F \) such that \( 1v = v \) for all \( v \in V \).
        \end{enumerate}
    \end{enumerate}
    
    \end{definition}
    
    
    
    \begin{definition}[Subspace]
        Let $V$ be a vector space, and let $W$ be a subset of $V$. We define $W$ to be a \textit{subspace} if $W$ satisfies the following conditions:
    \begin{enumerate}
        \item If $v, w$ are elements of $W$, their sum $v + w$ is also an element of $W$.
        \item If $v$ is an element of $W$ and $c$ is a scalar, then $cv$ is an element of $W$.
        \item The element $O$ of $V$ is also an element of $W$.
    \end{enumerate}
        Then $W$ itself is a vector space. Indeed, properties $\textbf{VS1}$ through $\textbf{VS8}$, being satisfied for all elements of $V$, are satisfied \textit{a fortiori} for the elements of $W$.
    \end{definition}
        
    
    
    
    
    \begin{definition}[Linear Combination]
        Let $V$ be an arbitrary vector space, and let $v_1, \dots, v_n$ be elements of $V$. Let $x_1, \dots, x_n$ be scalars. An expression of the form
        \[
        x_1 v_1 + \dots + x_n v_n
        \]
        is called a \textit{linear combination} of $v_1, \dots, v_n$.
    \end{definition}
        
    
    \begin{definition}[Dot Product]
        Let $V = K^n$. Let $A, B \in K^n$ with $A = (a_1, \dots, a_n)$ and $B = (b_1, \dots, b_n)$. We define the \textit{dot product} or \textit{scalar product} as
        \[
        A \cdot B = a_1 b_1 + \dots + a_n b_n.
        \]
    \end{definition}
        
    \begin{remark}
    Geometrically we say that $A$ and $B$ are orthogonal
    MORE HERE
    \end{remark}
    
    
    
    \begin{definition}[Linear Independence]
        Let $v_1, \dots, v_n$ be vectors in a vector space. The set of vectors $\{ v_1, \dots, v_n \}$ is said to be \textit{linearly independent} if the only solution to the equation
        \[
        a_1 v_1 + \dots + a_n v_n = O
        \]
        is $a_1 = a_2 = \dots = a_n = 0$. That is, the vectors are linearly independent if no nontrivial linear combination of them results in the zero vector.
    \end{definition}
        
    
    
    
    \begin{definition}[Basis]
        Let $V$ be a vector space. A set of vectors $\{ v_1, \dots, v_n \}$ in $V$ is called a \textit{basis} of $V$ if:
        \begin{enumerate}
            \item The vectors $v_1, \dots, v_n$ \textit{generate} $V$, meaning that every vector in $V$ can be written as a linear combination of $v_1, \dots, v_n$.
            \item The vectors $v_1, \dots, v_n$ are \textit{linearly independent}, meaning that the only solution to 
            \[
            a_1 v_1 + \dots + a_n v_n = O
            \]
            is $a_1 = a_2 = \dots = a_n = 0$.
    \end{enumerate}
        If these conditions are satisfied, we say that $\{ v_1, \dots, v_n \}$ \textit{forms a basis} of $V$.
    \end{definition}
        
    
    
    \begin{theorem}
        Let $V$ be a vector space. Let $v_1, \dots, v_n$ be linearly independent elements of $V$. Let $x_1, \dots, x_n$ and $y_1, \dots, y_n$ be scalars. Suppose that 
        \[
        x_1 v_1 + \dots + x_n v_n = y_1 v_1 + \dots + y_n v_n.
        \]
        Then $x_i = y_i$ for all $i = 1, \dots, n$.
    \end{theorem}
        
    
    
    \begin{theorem}
        Let $\{ v_1, \dots, v_n \}$ be a set of generators of a vector space $V$. Let $\{ v_1, \dots, v_r \}$ be a maximal subset of linearly independent elements. Then $\{ v_1, \dots, v_r \}$ is a basis of $V$.
    \end{theorem}
        
    
    
    
    
    \begin{definition}[Dimension of a Vector Space]
        Let $V$ be a vector space having a basis consisting of $n$ elements. We define $n$ to be the \textit{dimension} of $V$. If $V$ consists only of the zero vector $O$, then $V$ does not have a basis, and we define the dimension of $V$ to be $0$.
    \end{definition}
        
    
    
    
    
    
    
    \begin{theorem} \label{thm:maximal_independent_set}
        Let \( V \) be a vector space, and \( \{ v_1, \dots, v_n \} \) a maximal set of linearly independent elements of \( V \). Then \( \{ v_1, \dots, v_n \} \) is a basis of \( V \).
        \end{theorem}
        
        \begin{theorem} \label{thm:dim_n_basis}
        Let \( V \) be a vector space of dimension \( n \), and let \( v_1, \dots, v_n \) be linearly independent elements of \( V \). Then \( v_1, \dots, v_n \) constitute a basis of \( V \).
        \end{theorem}
        
        \begin{proof}
        According to Theorem \ref{thm:maximal_independent_set}, \( \{ v_1, \dots, v_n \} \) is a maximal set of linearly independent elements of \( V \). Hence it is a basis by Theorem \ref{thm:maximal_independent_set}.
        \end{proof}
        
        \begin{corollary} \label{cor:equal_dimension}
        Let \( V \) be a vector space and let \( W \) be a subspace. If \( \dim W = \dim V \), then \( V = W \).
        \end{corollary}
        
        \begin{proof}
        A basis for \( W \) must also be a basis for \( V \) by Theorem \ref{thm:dim_n_basis}.
        \end{proof}
        
        \begin{corollary} \label{cor:extend_basis}
        Let \( V \) be a vector space of dimension \( n \). Let \( r \) be a positive integer with \( r < n \), and let \( v_1, \dots, v_r \) be linearly independent elements of \( V \). Then one can find elements \( v_{r+1}, \dots, v_n \) such that
        \[
        \{ v_1, \dots, v_n \}
        \]
        is a basis of \( V \).
        \end{corollary}
        
        \begin{theorem} \label{thm:subspace_basis}
        Let \( V \) be a vector space having a basis consisting of \( n \) elements. Let \( W \) be a subspace which does not consist of \( O \) alone. Then \( W \) has a basis, and the dimension of \( W \) is \( \leq n \).
        \end{theorem}
        
        \begin{proof}
        Let \( w_1 \) be a nonzero element of \( W \). If \( \{ w_1 \} \) is not a maximal set of linearly independent elements of \( W \), we can find an element \( w_2 \) of \( W \) such that \( w_1, w_2 \) are linearly independent. Proceeding in this manner, one element at a time, there must be an integer \( m \leq n \) such that we can find linearly independent elements \( w_1, w_2, \dots, w_m \), and such that
        \[
        \{ w_1, \dots, w_m \}
        \]
        is a maximal set of linearly independent elements of \( W \) (by Theorem \ref{thm:maximal_independent_set}, we cannot go on indefinitely finding linearly independent elements, and the number of such elements is at most \( n \)). If we now use Theorem \ref{thm:maximal_independent_set}, we conclude that \( \{ w_1, \dots, w_m \} \) is a basis for \( W \).
        \end{proof}
        
        \begin{definition} \label{def:sum_subspaces}
        Let \( V \) be a vector space over the field \( K \). Let \( U, W \) be subspaces of \( V \). We define the \textit{sum} of \( U \) and \( W \) to be the subset of \( V \) consisting of all sums \( u + w \) with \( u \in U \) and \( w \in W \). We denote this sum by \( U + W \). It is a subspace of \( V \). Indeed, if \( u_1, u_2 \in U \) and \( w_1, w_2 \in W \) then
        \[
        (u_1 + w_1) + (u_2 + w_2) = u_1 + u_2 + w_1 + w_2 \in U + W.
        \]
        If \( c \in K \), then
        \[
        c (u_1 + w_1) = c u_1 + c w_1 \in U + W.
        \]
        Finally, \( O + O \in W \). This proves that \( U + W \) is a subspace.
        
        We shall say that \( V \) is a \textit{direct sum} of \( U \) and \( W \) if for every element \( v \) of \( V \) there exist \textit{unique} elements \( u \in U \) and \( w \in W \) such that \( v = u + w \).
        \end{definition}
        
        \begin{theorem} \label{thm:direct_sum_condition}
        Let \( V \) be a vector space over the field \( K \), and let \( U, W \) be subspaces. If \( U + W = V \), and if \( U \cap W = \{ O \} \), then \( V \) is the \textit{direct sum} of \( U \) and \( W \).
        \end{theorem}
        
        \begin{theorem} \label{thm:direct_sum_existence}
        Let \( V \) be a finite-dimensional vector space over the field \( K \). Let \( W \) be a subspace. Then there exists a subspace \( U \) such that \( V \) is the direct sum of \( W \) and \( U \).
        \end{theorem}
        
        \begin{theorem} \label{thm:direct_sum_dimension}
        If \( V \) is a finite-dimensional vector space over \( K \), and is the direct sum of subspaces \( U, W \), then
        \[
        \dim V = \dim U + \dim W.
        \]
        \end{theorem}
        
    
    
    
    
    
    
    
    
    
    
    
    
    
    

\end{document}